{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment3_Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Stefanosdl/Neural_Networks_and_clustering_of_images/blob/main/Assignment3_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxI19ruBE9Y7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df8b073a-466c-4865-8687-b71b53625d7b"
      },
      "source": [
        "%tensorflow_version 2.x\r\n",
        "import tensorflow as tf\r\n",
        "# print(\"Tensorflow version \" + tf.__version__)\r\n",
        "\r\n",
        "try:\r\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\r\n",
        "#   print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\r\n",
        "except ValueError:\r\n",
        "  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\r\n",
        "\r\n",
        "tf.config.experimental_connect_to_cluster(tpu)\r\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\r\n",
        "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.46.26.218:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.46.26.218:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n",
            "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7T6kt4VFUM5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a4f4177-9abb-47ef-d10b-16dede35d658"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCFfb2G1FVzM"
      },
      "source": [
        "#Datasets\r\n",
        "path = \"/content/drive/My Drive/Project_Ergasia3/Datasets/\"\r\n",
        "inputfile = path + \"t10k-images.idx3-ubyte\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuDMF8LlVRQs"
      },
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHTqsUFhVlSb"
      },
      "source": [
        "def reader(dataset):\n",
        "    # create a dictionary of lists to store all images\n",
        "    all_images = {}\n",
        "    all_images[0] = []\n",
        "    with open(dataset, \"rb\") as f:\n",
        "        counter = 0\n",
        "        # read metadata\n",
        "        byte = f.read(4)\n",
        "        while byte:\n",
        "            if counter == 0:\n",
        "                magic_number = int.from_bytes(byte, \"big\")\n",
        "            elif counter == 1:\n",
        "                number_of_images = int.from_bytes(byte, \"big\")\n",
        "            elif counter == 2:\n",
        "                rows = int.from_bytes(byte, \"big\")\n",
        "            elif counter == 3:\n",
        "                cols = int.from_bytes(byte, \"big\")\n",
        "                break\n",
        "            counter += 1\n",
        "            byte = f.read(4)\n",
        "        # start reading the images \n",
        "        byte_counter = 0\n",
        "        image_counter = 0\n",
        "        dimensions = rows * cols\n",
        "        print (magic_number, \" \", number_of_images,\" \", rows,\" \", cols, \" \", dimensions)\n",
        "        # read images, byte byte\n",
        "        byte = f.read(1)\n",
        "        while byte:\n",
        "            # store byte in the \n",
        "            all_images[image_counter].append(int.from_bytes(byte, \"big\"))\n",
        "            byte_counter += 1\n",
        "            if (byte_counter == dimensions):\n",
        "                # next image\n",
        "                image_counter += 1\n",
        "                byte_counter = 0\n",
        "                # initialize the list for this image\n",
        "                all_images[image_counter] = []\n",
        "            byte = f.read(1)\n",
        "    # finished with reading of file\n",
        "    # remove last item number_of_images index that is anyway an empty list\n",
        "    all_images.popitem()\n",
        "    print (\"finished reading from file\")\n",
        "    # return a tuple of number of images, dimensions and all_images dict\n",
        "    return all_images\n",
        "\n",
        "def reader_labels(dataset):\n",
        "    # create a dictionary of lists to store all images\n",
        "    all_labels = []\n",
        "    with open(dataset, \"rb\") as f:\n",
        "        counter = 0\n",
        "        # read metadata\n",
        "        byte = f.read(4)\n",
        "        while byte:\n",
        "            if counter == 0:\n",
        "                magic_number = int.from_bytes(byte, \"big\")\n",
        "            elif counter == 1:\n",
        "                number_of_items = int.from_bytes(byte, \"big\")\n",
        "                break\n",
        "            counter += 1\n",
        "            byte = f.read(4)\n",
        "\n",
        "\t\t# start reading the labels \n",
        "        byte = f.read(1)\n",
        "        while byte:\n",
        "\t\t\t# store byte in the \n",
        "            all_labels.append(int.from_bytes(byte, \"big\"))\n",
        "            byte = f.read(1)\n",
        "            # finished with reading of file\n",
        "    return all_labels\n",
        "\n",
        "def read_all_files(training_set, training_labels, test_set, test_labels):\n",
        "\ttrain_images = reader(training_set)\n",
        "\ttrain_labels = reader_labels(training_labels) \n",
        "\ttest_images = reader(test_set)\n",
        "\ttest_labels = reader_labels(test_labels)\n",
        "\t# labels are lists of 10000 items\n",
        "\treturn (train_images, train_labels, test_images, test_labels)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QT4I1doBVtkw"
      },
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "from matplotlib import pyplot as plt\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras import backend as K\n",
        "from keras.layers.embeddings import Embedding\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9UOdyGMWgMD"
      },
      "source": [
        "def convolutionalAutoencoder():\r\n",
        "    keras.backend.clear_session()\r\n",
        "    print(\"[INFO] building autoencoder...\")\r\n",
        "    model = tf.keras.models.Sequential()\r\n",
        "    model.add(layers.Input(shape=(28, 28, 1)))\r\n",
        "    model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same', strides=2))\r\n",
        "    model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same', strides=2))\r\n",
        "    model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same', strides=3))\r\n",
        "\r\n",
        "    model.add(layers.Flatten())\r\n",
        "    model.add(layers.Dense(10, activation='relu'))\r\n",
        "    # model.add(layers.Dropout(0.3))\r\n",
        "    model.add(layers.Dense(1152, activation='relu'))\r\n",
        "\r\n",
        "    model.add(layers.Reshape((3, 3, 128)))\r\n",
        "\r\n",
        "    model.add(layers.Conv2DTranspose(64, kernel_size=3, strides=2, activation='relu'))\r\n",
        "    model.add(layers.Conv2DTranspose(32, kernel_size=3, strides=2, activation='relu', padding='same'))\r\n",
        "    model.add(layers.Conv2DTranspose(1, kernel_size=3, strides=2, activation='sigmoid', padding='same'))\r\n",
        "\r\n",
        "    return model\r\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjiB8kXbWwrU"
      },
      "source": [
        "# training \r\n",
        "def training(autoencoder, all_images, epochs=10, batch_size=128):\r\n",
        "\tprint(\"[INFO] training started...\")\r\n",
        "\tdf = pd.DataFrame.from_dict(all_images, orient='index')\r\n",
        "\t# now in df we have a dataframe with size: dimensions x number_of_images with all of our images\r\n",
        "\topt = keras.optimizers.Adam(lr=1e-3)\r\n",
        "\t# loss = 'binary_crossentropy'\r\n",
        "\tautoencoder.compile(loss=\"mse\", optimizer=opt, metrics = ['accuracy'])\r\n",
        "\r\n",
        "\t# To train it we will need the data\r\n",
        "\ttrain, test = train_test_split(df, test_size=0.33, random_state=42)\r\n",
        "\r\n",
        "\ttrain = train.astype('float32') / 255.\r\n",
        "\ttest = test.astype('float32') / 255.\r\n",
        "\r\n",
        "\ttrain = train.to_numpy()\r\n",
        "\ttest = test.to_numpy()\r\n",
        "\r\n",
        "\ttrain = np.reshape(train, (len(train), 28, 28, 1))\r\n",
        "\ttest = np.reshape(test, (len(test), 28, 28, 1))\r\n",
        "\r\n",
        "\thistory = autoencoder.fit(train, train,\r\n",
        "\t\t\t\t\tepochs=epochs,\r\n",
        "\t\t\t\t\tbatch_size=batch_size,\r\n",
        "\t\t\t\t\tshuffle=True,\r\n",
        "\t\t\t\t\tvalidation_data=(test, test),\r\n",
        "\t\t\t\t\tcallbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)])\r\n",
        "\r\n",
        "\treturn (history, test)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Djk3y5KNW0BA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a028ef31-333b-4c10-a13f-2e2f88e3bda4"
      },
      "source": [
        "# run this\r\n",
        "epochs = 20\r\n",
        "autoencoder = convolutionalAutoencoder()\r\n",
        "all_images = reader(inputfile)\r\n",
        "(history, test) = training(autoencoder, all_images)\r\n",
        "autoencoder.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] building autoencoder...\n",
            "2051   10000   28   28   784\n",
            "finished reading from file\n",
            "[INFO] training started...\n",
            "Epoch 1/10\n",
            "53/53 [==============================] - 3s 40ms/step - loss: 0.1945 - accuracy: 0.7773 - val_loss: 0.1104 - val_accuracy: 0.8052\n",
            "Epoch 2/10\n",
            "53/53 [==============================] - 1s 27ms/step - loss: 0.1077 - accuracy: 0.8038 - val_loss: 0.1046 - val_accuracy: 0.8034\n",
            "Epoch 3/10\n",
            "53/53 [==============================] - 2s 31ms/step - loss: 0.1016 - accuracy: 0.8019 - val_loss: 0.0676 - val_accuracy: 0.7962\n",
            "Epoch 4/10\n",
            "53/53 [==============================] - 1s 28ms/step - loss: 0.0645 - accuracy: 0.8006 - val_loss: 0.0574 - val_accuracy: 0.7942\n",
            "Epoch 5/10\n",
            "53/53 [==============================] - 1s 27ms/step - loss: 0.0544 - accuracy: 0.7917 - val_loss: 0.0487 - val_accuracy: 0.7880\n",
            "Epoch 6/10\n",
            "53/53 [==============================] - 1s 27ms/step - loss: 0.0465 - accuracy: 0.7900 - val_loss: 0.0406 - val_accuracy: 0.7945\n",
            "Epoch 7/10\n",
            "53/53 [==============================] - 1s 27ms/step - loss: 0.0389 - accuracy: 0.7953 - val_loss: 0.0370 - val_accuracy: 0.7965\n",
            "Epoch 8/10\n",
            "53/53 [==============================] - 1s 27ms/step - loss: 0.0358 - accuracy: 0.7983 - val_loss: 0.0334 - val_accuracy: 0.8013\n",
            "Epoch 9/10\n",
            "53/53 [==============================] - 1s 27ms/step - loss: 0.0321 - accuracy: 0.8013 - val_loss: 0.0315 - val_accuracy: 0.7996\n",
            "Epoch 10/10\n",
            "53/53 [==============================] - 1s 28ms/step - loss: 0.0308 - accuracy: 0.8008 - val_loss: 0.0300 - val_accuracy: 0.8022\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 14, 14, 32)        320       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 7, 7, 64)          18496     \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 3, 3, 128)         73856     \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 1152)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                11530     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1152)              12672     \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 3, 3, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose (Conv2DTran (None, 7, 7, 64)          73792     \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTr (None, 14, 14, 32)        18464     \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_2 (Conv2DTr (None, 28, 28, 1)         289       \n",
            "=================================================================\n",
            "Total params: 209,419\n",
            "Trainable params: 209,419\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUnXChFmsAy6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f49a447f-4f64-4022-cb20-456353549560"
      },
      "source": [
        "encoder = tf.keras.models.Sequential()\r\n",
        "encoder.add(layers.Input(shape=(28, 28, 1)))\r\n",
        "for i in range(0, len(autoencoder.layers)//2):\r\n",
        "    encoder.add(autoencoder.layers[i])\r\n",
        "decoded_imgs=encoder.predict(test)\r\n",
        "print(encoder.summary())\r\n",
        "print(decoded_imgs.max())\r\n",
        "print(decoded_imgs.min())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 14, 14, 32)        320       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 7, 7, 64)          18496     \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 3, 3, 128)         73856     \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 1152)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                11530     \n",
            "=================================================================\n",
            "Total params: 104,202\n",
            "Trainable params: 104,202\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "88.93832\n",
            "0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_Ymyjnjba9d"
      },
      "source": [
        "# def normalize(x):\n",
        "#   return ((25500*(x - np.min(x))/np.ptp(x)).astype(int))\n",
        "\n",
        "normalized = []\n",
        "for i in range(len(decoded_imgs)):\n",
        "    normalized.append((25500*(decoded_imgs[i] - np.min(decoded_imgs[i]))/np.ptp(decoded_imgs[i])).astype(int))\n",
        "\n",
        "normalized = np.asarray(normalized)\n",
        "# normalized = dict(enumerate(normalized, 1)) "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zqTbOQR664q"
      },
      "source": [
        "# TODO: CHANGE THE PATH!!!!!!!!\r\n",
        "# outputfile = open(\"decoded_images\", \"wb\")\r\n",
        "\r\n",
        "# for key, value in normalized.items():\r\n",
        "#     # for pic_value in value:\r\n",
        "#     outputfile.write(bytearray(value))\r\n",
        "# outputfile.close()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0-rh3qGXZfW"
      },
      "source": [
        "import struct\n",
        "normalized.astype(np.ushort)\n",
        "# TODO: CHANGE THE PATH\n",
        "outputfile = open(\"images_new_space\",\"wb\")\n",
        "number_of_images = normalized.shape[0]\n",
        "dimensions = normalized.shape[1]\n",
        "# write metadata\n",
        "# enforce big endian\n",
        "myfmt = '>i'\n",
        "# write magic number\n",
        "bin = struct.pack(myfmt, 2081)\n",
        "outputfile.write(bin)\n",
        "# write num of images\n",
        "bin = struct.pack(myfmt, number_of_images)\n",
        "outputfile.write(bin)\n",
        "# write num of rows (1)\n",
        "bin = struct.pack(myfmt, 1)\n",
        "outputfile.write(bin)\n",
        "# write dimensions\n",
        "bin = struct.pack(myfmt, dimensions)\n",
        "outputfile.write(bin)\n",
        "# enforce little endian\n",
        "myfmt = 'H'*dimensions\n",
        "myfmt = '<' + myfmt\n",
        "#  You can use 'H' for unsigned short and <, for little endian to force endinness\n",
        "for i in range(0, number_of_images):\n",
        "    arr = np.array(normalized[i])\n",
        "    bin = struct.pack(myfmt, *arr)\n",
        "    outputfile.write(bin)\n",
        "\n",
        "outputfile.close()\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFTccctj-Awb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5a999ba-3758-4183-d8e7-37db30ef3b6f"
      },
      "source": [
        "import os\n",
        "os.path.getsize('images_new_space')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66016"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    }
  ]
}